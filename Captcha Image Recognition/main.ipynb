{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0zuYQzwiO8Z",
        "colab_type": "text"
      },
      "source": [
        "# M2608.001300 Machine Learning<br> Assignment #5 Final Projects (Pytorch)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ri0oOICuC64e",
        "colab_type": "text"
      },
      "source": [
        "Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJfG1l3RC_c3",
        "colab_type": "text"
      },
      "source": [
        "**For understanding of this work, please carefully look at given PPT file.**\n",
        "\n",
        "Note: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9yv4oGGDbmJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70d1e557-5657-46cd-f5fd-534a01dc5062"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "from torch.autograd import Variable\n",
        "from PIL import Image\n",
        "#import resnet\n",
        "import torchvision.models as models\n",
        "\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense, Activation"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYj_QhOCsY6K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acvGcUAaEkxe",
        "colab_type": "text"
      },
      "source": [
        "Load datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcPk4u8qGZHB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a5f32bbc-b26b-4e90-cb30-86cd3799acec"
      },
      "source": [
        "NUMBER = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
        "ALPHABET = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
        "NONE = ['NONE'] # label for empty space\n",
        "ALL_CHAR_SET = NUMBER + ALPHABET + NONE\n",
        "ALL_CHAR_SET_LEN = len(ALL_CHAR_SET)\n",
        "MAX_CAPTCHA = 7\n",
        "\n",
        "print(ALL_CHAR_SET.index('NONE'))\n",
        "\n",
        "def encode(a):\n",
        "    onehot = [0]*ALL_CHAR_SET_LEN\n",
        "    idx = ALL_CHAR_SET.index(a)\n",
        "    onehot[idx] += 1\n",
        "    return onehot\n",
        "\n",
        "# modified dataset class\n",
        "class Mydataset(Dataset):\n",
        "    def __init__(self, img_path, label_path, is_train=True, transform=None):\n",
        "        self.path = img_path\n",
        "        self.label_path = label_path\n",
        "        if is_train: \n",
        "            self.img = os.listdir(self.path)[:10000]\n",
        "            self.labels = open(self.label_path, 'r').read().split('\\n')[:-1][:10000]\n",
        "        else: \n",
        "            self.img = os.listdir(self.path)[:1000]\n",
        "            self.labels = open(self.label_path, 'r').read().split('\\n')[:-1][:1000]\n",
        "        \n",
        "        self.transform = transform\n",
        "        self.max_length = MAX_CAPTCHA\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.img[idx]\n",
        "        img = Image.open(f'{self.path}/{self.img[idx]}')\n",
        "        img = img.convert('L')\n",
        "        label = self.labels[idx]\n",
        "        label_oh = []\n",
        "        # one-hot for each character\n",
        "        for i in range(self.max_length):\n",
        "            if i < len(label):\n",
        "                label_oh += encode(label[i])\n",
        "            else:\n",
        "                #label_oh += [0]*ALL_CHAR_SET_LEN\n",
        "                label_oh += encode('NONE')\n",
        "            \n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "        return img, np.array(label_oh), label\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.img)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize([160, 60]),\n",
        "    transforms.ToTensor(),\n",
        "##############################################################################\n",
        "#                          IMPLEMENT YOUR CODE                               #\n",
        "##############################################################################\n",
        "#    transforms.Normalize(mean=0.456, std=0.224)\n",
        "##############################################################################\n",
        "#                          END OF YOUR CODE                                  #\n",
        "##############################################################################\n",
        "])\n",
        "\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtXUV9W9oLbc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EFtlQyubGJZb",
        "colab": {}
      },
      "source": [
        "\"\"\"Loading DATA\"\"\"\n",
        "# Change to your own data foler path!\n",
        "gPath = '/content/drive/My Drive/2020Spring_ML_final/'\n",
        "\n",
        "train_ds = Mydataset(gPath+'Data/train/', gPath+'Data/train.txt',transform=transform)\n",
        "test_ds = Mydataset(gPath+'Data/test/', gPath+'Data/test.txt',False, transform)\n",
        "train_dl = DataLoader(train_ds, batch_size=128, num_workers=4)\n",
        "test_dl = DataLoader(test_ds, batch_size=1, num_workers=4)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka5SgX6VIWcG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"To CUDA for local run\"\"\"\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#GPUID = '4' # define GPUID\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPUID)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJaHW3wSENjY",
        "colab_type": "text"
      },
      "source": [
        "Problem 1: Design LSTM model for catcha image recognition. (10 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rHEe3XmBFQHq",
        "colab": {}
      },
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, cnn_dim, hidden_size, vocab_size, num_layers=1):\n",
        "        super(LSTM, self).__init__()\n",
        "        \n",
        "        # define the properties\n",
        "        self.cnn_dim = cnn_dim\n",
        "        self.hidden_size = hidden_size\n",
        "        self.vocab_size = vocab_size\n",
        "        \n",
        "        # lstm cell\n",
        "        self.lstm_cell = nn.LSTMCell(input_size=self.vocab_size, hidden_size=hidden_size)\n",
        "    \n",
        "        # output fully connected layer\n",
        "        self.fc_in = nn.Linear(in_features=self.cnn_dim, out_features=self.vocab_size)\n",
        "        self.fc_out = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)\n",
        "    \n",
        "        # embedding layer \n",
        "        self.embed = nn.Embedding(num_embeddings=self.vocab_size, embedding_dim=self.vocab_size)\n",
        "    \n",
        "        # activations\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "    \n",
        "    def forward(self, features, captions):\n",
        "\n",
        "        batch_size = features.size(0)\n",
        "        cnn_dim = features.size(1)\n",
        "\n",
        "        hidden_state = torch.zeros((batch_size, self.hidden_size)).cuda()\n",
        "        cell_state = torch.zeros((batch_size, self.hidden_size)).cuda()\n",
        "    \n",
        "        # define the output tensor placeholder\n",
        "        outputs = torch.empty((batch_size, captions.size(1), self.vocab_size)).cuda()\n",
        "\n",
        "        # embed the captions\n",
        "        captions_embed = self.embed(captions)\n",
        "\n",
        "##############################################################################\n",
        "#                          IMPLEMENT YOUR CODE                               #\n",
        "##############################################################################\n",
        "        for t in range(captions.size(1)):\n",
        "          if t == 0:\n",
        "            hidden_state, cell_state=self.lstm_cell(features, (hidden_state, cell_state))\n",
        "          else:\n",
        "            hidden_state, cell_state=self.lstm_cell(captions_embed[:,t,:], (hidden_state,cell_state))\n",
        "\n",
        "          out = self.fc_out(hidden_state)\n",
        "\n",
        "          outputs[:, t, :]=out\n",
        "##############################################################################\n",
        "#                          END OF YOUR CODE                                  #\n",
        "##############################################################################\n",
        "        return outputs \n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TM2vL2PTFeEt",
        "colab_type": "text"
      },
      "source": [
        "Problem 2: \n",
        "\n",
        "*   1.Connect CNN model to the desinged LSTM model.\n",
        "*   2.Replace ResNet to your own CNN model from Assignment3.\n",
        "\n",
        "\n",
        "          \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwgpQ1aiFq2a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "##############################################################################\n",
        "#                          IMPLEMENT YOUR CODE                               #\n",
        "##############################################################################\n",
        " #CNN\n",
        "\n",
        "class BetterNet(nn.Module):\n",
        "    def __init__(self,embed_size=ALL_CHAR_SET_LEN*MAX_CAPTCHA):\n",
        "        super(BetterNet, self).__init__()\n",
        "        \n",
        "        self.densenet = models.densenet121(pretrained=True)\n",
        "        self.densenet.features.conv0 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "        \n",
        "        # replace the classifier with a fully connected embedding layer\n",
        "        self.densenet.classifier = nn.Linear(in_features=1024, out_features=512)\n",
        "        \n",
        "        # add another fully connected layer\n",
        "        self.embed = nn.Linear(in_features=512, out_features=embed_size)\n",
        "        \n",
        "        # dropout layer\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "        \n",
        "        # activation layers\n",
        "        self.prelu = nn.PReLU()\n",
        "        \n",
        "    def forward(self, images):\n",
        "        \n",
        "        # get the embeddings from the densenet\n",
        "        out = self.prelu(self.densenet(images))\n",
        "       # out = self.dropout(out)\n",
        "        # pass through the fully connected\n",
        "        out = self.embed(out)\n",
        "        \n",
        "        return out\n",
        "#CNN\n",
        "betternet = BetterNet() \n",
        "\"\"\"\n",
        "#Resnet\n",
        "betternet = resnet.resnet18(pretrained=False)\n",
        "betternet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
        "betternet.fc = nn.Linear(in_features=512, out_features=ALL_CHAR_SET_LEN*MAX_CAPTCHA, bias=True)\n",
        "\"\"\"\n",
        "betternet = betternet.to(device)\n",
        "\n",
        "##############################################################################\n",
        "#                          END OF YOUR CODE                                  #\n",
        "##############################################################################\n",
        "\n",
        "       \n",
        "# LSTM\n",
        "cnn_dim=512 #resnet18-512\n",
        "hidden_size=8\n",
        "vocab_size =ALL_CHAR_SET_LEN*MAX_CAPTCHA # 수정\n",
        "lstm = LSTM(cnn_dim=cnn_dim, hidden_size=hidden_size, vocab_size=vocab_size)\n",
        "lstm = lstm.to(device)\n",
        "\n",
        "# loss, optimizer 이거를 바꾸자\n",
        "##############################################################################\n",
        "#                          IMPLEMENT YOUR CODE                               #\n",
        "##############################################################################\n",
        "loss_func = nn.MultiLabelSoftMarginLoss()\n",
        "cnn_optim = torch.optim.SGD(betternet.parameters(), lr=0.00025,momentum=0.9)\n",
        "#lstm_optim = torch.optim.Adam(lstm.parameters(), lr=0.001)\n",
        "##############################################################################\n",
        "#                          END OF YOUR CODE                                  #\n",
        "##############################################################################"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoeTIkXjHJIE",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0uCexwRHsNz",
        "colab_type": "text"
      },
      "source": [
        "Problem3: Find hyper-parameters.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ibfVzKZeH1yC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "477a8928-4cf9-4394-a212-f3b9f60834d6"
      },
      "source": [
        "\"\"\"TRAINING\"\"\"\n",
        "print_interval = 15\n",
        "max_epoch = 100\n",
        "\n",
        "for epoch in range(max_epoch):\n",
        "    for step, i in enumerate(train_dl):\n",
        "        img, label_oh, label = i\n",
        "        img = Variable(img).cuda()\n",
        "#        label_oh_train=label_oh[:,:label_oh.shape[1]-1].to(device) # lstm연결을 위한 것 추가\n",
        "#        label_oh_target=label_oh[:,1:].to(device) # lstm연결을 위한 것 추가\n",
        "        label_oh = Variable(label_oh.float()).cuda()\n",
        "        batch_size, _ = label_oh.shape\n",
        "        pred = betternet(img)\n",
        "#        pred = lstm(pred, label_oh)\n",
        "        loss = loss_func(pred, label_oh)\n",
        "        cnn_optim.zero_grad()\n",
        "        loss.backward()\n",
        "        cnn_optim.step()\n",
        "##############################################################################\n",
        "#                          IMPLEMENT YOUR CODE                               #\n",
        "############################################################################## \n",
        "#        betternet.zero_grad()\n",
        "#        lstm.zero_grad()\n",
        "        \n",
        "#        betternet.train()\n",
        "#        lstm.train()\n",
        "\n",
        "         # cnn-lstm 결합\n",
        "#        pred = lstm(pred, label_oh_train)\n",
        "#        if lstm_optim is not None:\n",
        "#            lstm_optim.zero_grad()                \n",
        "#        if lstm_optim is not None:\n",
        "#            lstm_optim.step()\n",
        "# model은 위에서 합친 모델 - 아마 feature를 lstm으로 받는거겠지 그럼 loss도 다시바꿔야겠다\n",
        "# 파라미터를 바꾸는 건데 뭘 바꿔야하는지 모르겠다 이대로 돌려도 되긴해\n",
        "# 위 파라미터 값 수정, 여기서 모델을 합쳐도 됨\n",
        "\n",
        "##############################################################################\n",
        "#                          END OF YOUR CODE                                  #\n",
        "##############################################################################\n",
        "        if (step+1)%print_interval == 0:\n",
        "            print('epoch:', epoch+1, 'step:', step+1, 'loss:', loss.item())"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch: 1 step: 15 loss: 0.6994454860687256\n",
            "epoch: 1 step: 30 loss: 0.6959024667739868\n",
            "epoch: 1 step: 45 loss: 0.6929036378860474\n",
            "epoch: 1 step: 60 loss: 0.6894955635070801\n",
            "epoch: 1 step: 75 loss: 0.6859017610549927\n",
            "epoch: 2 step: 15 loss: 0.6819623112678528\n",
            "epoch: 2 step: 30 loss: 0.6782490611076355\n",
            "epoch: 2 step: 45 loss: 0.6752631664276123\n",
            "epoch: 2 step: 60 loss: 0.6718862056732178\n",
            "epoch: 2 step: 75 loss: 0.6683573722839355\n",
            "epoch: 3 step: 15 loss: 0.6643806099891663\n",
            "epoch: 3 step: 30 loss: 0.6606832146644592\n",
            "epoch: 3 step: 45 loss: 0.6576210260391235\n",
            "epoch: 3 step: 60 loss: 0.6541764736175537\n",
            "epoch: 3 step: 75 loss: 0.650532066822052\n",
            "epoch: 4 step: 15 loss: 0.6463741660118103\n",
            "epoch: 4 step: 30 loss: 0.6425571441650391\n",
            "epoch: 4 step: 45 loss: 0.6392848491668701\n",
            "epoch: 4 step: 60 loss: 0.6356703042984009\n",
            "epoch: 4 step: 75 loss: 0.6317853331565857\n",
            "epoch: 5 step: 15 loss: 0.6272999048233032\n",
            "epoch: 5 step: 30 loss: 0.6232743859291077\n",
            "epoch: 5 step: 45 loss: 0.6196931600570679\n",
            "epoch: 5 step: 60 loss: 0.6158190369606018\n",
            "epoch: 5 step: 75 loss: 0.6116011142730713\n",
            "epoch: 6 step: 15 loss: 0.6066635847091675\n",
            "epoch: 6 step: 30 loss: 0.6023623943328857\n",
            "epoch: 6 step: 45 loss: 0.5983919501304626\n",
            "epoch: 6 step: 60 loss: 0.5942091941833496\n",
            "epoch: 6 step: 75 loss: 0.5895748734474182\n",
            "epoch: 7 step: 15 loss: 0.5841115117073059\n",
            "epoch: 7 step: 30 loss: 0.5794672966003418\n",
            "epoch: 7 step: 45 loss: 0.5750784873962402\n",
            "epoch: 7 step: 60 loss: 0.5705795288085938\n",
            "epoch: 7 step: 75 loss: 0.5654489398002625\n",
            "epoch: 8 step: 15 loss: 0.5594160556793213\n",
            "epoch: 8 step: 30 loss: 0.5543848276138306\n",
            "epoch: 8 step: 45 loss: 0.5495889186859131\n",
            "epoch: 8 step: 60 loss: 0.5447861552238464\n",
            "epoch: 8 step: 75 loss: 0.5391615033149719\n",
            "epoch: 9 step: 15 loss: 0.5325312614440918\n",
            "epoch: 9 step: 30 loss: 0.5271264910697937\n",
            "epoch: 9 step: 45 loss: 0.5219454765319824\n",
            "epoch: 9 step: 60 loss: 0.5168717503547668\n",
            "epoch: 9 step: 75 loss: 0.5107446908950806\n",
            "epoch: 10 step: 15 loss: 0.5035838484764099\n",
            "epoch: 10 step: 30 loss: 0.497845858335495\n",
            "epoch: 10 step: 45 loss: 0.49233779311180115\n",
            "epoch: 10 step: 60 loss: 0.48707181215286255\n",
            "epoch: 10 step: 75 loss: 0.48043471574783325\n",
            "epoch: 11 step: 15 loss: 0.47291678190231323\n",
            "epoch: 11 step: 30 loss: 0.4668499231338501\n",
            "epoch: 11 step: 45 loss: 0.46116209030151367\n",
            "epoch: 11 step: 60 loss: 0.4557960629463196\n",
            "epoch: 11 step: 75 loss: 0.4487308859825134\n",
            "epoch: 12 step: 15 loss: 0.44101661443710327\n",
            "epoch: 12 step: 30 loss: 0.43468281626701355\n",
            "epoch: 12 step: 45 loss: 0.4289889931678772\n",
            "epoch: 12 step: 60 loss: 0.4236053228378296\n",
            "epoch: 12 step: 75 loss: 0.4162195324897766\n",
            "epoch: 13 step: 15 loss: 0.4085144102573395\n",
            "epoch: 13 step: 30 loss: 0.40207791328430176\n",
            "epoch: 13 step: 45 loss: 0.3964471220970154\n",
            "epoch: 13 step: 60 loss: 0.3912501335144043\n",
            "epoch: 13 step: 75 loss: 0.3836573362350464\n",
            "epoch: 14 step: 15 loss: 0.3761608600616455\n",
            "epoch: 14 step: 30 loss: 0.3697492182254791\n",
            "epoch: 14 step: 45 loss: 0.36433306336402893\n",
            "epoch: 14 step: 60 loss: 0.35946083068847656\n",
            "epoch: 14 step: 75 loss: 0.35178157687187195\n",
            "epoch: 15 step: 15 loss: 0.344710111618042\n",
            "epoch: 15 step: 30 loss: 0.33846020698547363\n",
            "epoch: 15 step: 45 loss: 0.333355575799942\n",
            "epoch: 15 step: 60 loss: 0.3289366662502289\n",
            "epoch: 15 step: 75 loss: 0.3213161826133728\n",
            "epoch: 16 step: 15 loss: 0.3148305416107178\n",
            "epoch: 16 step: 30 loss: 0.30884233117103577\n",
            "epoch: 16 step: 45 loss: 0.304118812084198\n",
            "epoch: 16 step: 60 loss: 0.3002563714981079\n",
            "epoch: 16 step: 75 loss: 0.2928279638290405\n",
            "epoch: 17 step: 15 loss: 0.2870127558708191\n",
            "epoch: 17 step: 30 loss: 0.28138479590415955\n",
            "epoch: 17 step: 45 loss: 0.2771068215370178\n",
            "epoch: 17 step: 60 loss: 0.2738347351551056\n",
            "epoch: 17 step: 75 loss: 0.2667137384414673\n",
            "epoch: 18 step: 15 loss: 0.26161980628967285\n",
            "epoch: 18 step: 30 loss: 0.2564130127429962\n",
            "epoch: 18 step: 45 loss: 0.252585768699646\n",
            "epoch: 18 step: 60 loss: 0.2499280869960785\n",
            "epoch: 18 step: 75 loss: 0.24318335950374603\n",
            "epoch: 19 step: 15 loss: 0.23883606493473053\n",
            "epoch: 19 step: 30 loss: 0.2340582013130188\n",
            "epoch: 19 step: 45 loss: 0.23067587614059448\n",
            "epoch: 19 step: 60 loss: 0.22858396172523499\n",
            "epoch: 19 step: 75 loss: 0.222251296043396\n",
            "epoch: 20 step: 15 loss: 0.2186557948589325\n",
            "epoch: 20 step: 30 loss: 0.21429315209388733\n",
            "epoch: 20 step: 45 loss: 0.2113247811794281\n",
            "epoch: 20 step: 60 loss: 0.20974101126194\n",
            "epoch: 20 step: 75 loss: 0.20381778478622437\n",
            "epoch: 21 step: 15 loss: 0.20096269249916077\n",
            "epoch: 21 step: 30 loss: 0.19699004292488098\n",
            "epoch: 21 step: 45 loss: 0.19441360235214233\n",
            "epoch: 21 step: 60 loss: 0.19326430559158325\n",
            "epoch: 21 step: 75 loss: 0.1877521276473999\n",
            "epoch: 22 step: 15 loss: 0.18555690348148346\n",
            "epoch: 22 step: 30 loss: 0.18196704983711243\n",
            "epoch: 22 step: 45 loss: 0.17972368001937866\n",
            "epoch: 22 step: 60 loss: 0.17893086373806\n",
            "epoch: 22 step: 75 loss: 0.17381224036216736\n",
            "epoch: 23 step: 15 loss: 0.1722099930047989\n",
            "epoch: 23 step: 30 loss: 0.1689649522304535\n",
            "epoch: 23 step: 45 loss: 0.16702520847320557\n",
            "epoch: 23 step: 60 loss: 0.16652077436447144\n",
            "epoch: 23 step: 75 loss: 0.16177043318748474\n",
            "epoch: 24 step: 15 loss: 0.16069304943084717\n",
            "epoch: 24 step: 30 loss: 0.15774638950824738\n",
            "epoch: 24 step: 45 loss: 0.15606650710105896\n",
            "epoch: 24 step: 60 loss: 0.15579605102539062\n",
            "epoch: 24 step: 75 loss: 0.15138712525367737\n",
            "epoch: 25 step: 15 loss: 0.1507604420185089\n",
            "epoch: 25 step: 30 loss: 0.14808014035224915\n",
            "epoch: 25 step: 45 loss: 0.14661769568920135\n",
            "epoch: 25 step: 60 loss: 0.1465366780757904\n",
            "epoch: 25 step: 75 loss: 0.1424320638179779\n",
            "epoch: 26 step: 15 loss: 0.14218993484973907\n",
            "epoch: 26 step: 30 loss: 0.13975122570991516\n",
            "epoch: 26 step: 45 loss: 0.13845942914485931\n",
            "epoch: 26 step: 60 loss: 0.13854113221168518\n",
            "epoch: 26 step: 75 loss: 0.1347099393606186\n",
            "epoch: 27 step: 15 loss: 0.13478980958461761\n",
            "epoch: 27 step: 30 loss: 0.1325562447309494\n",
            "epoch: 27 step: 45 loss: 0.13139931857585907\n",
            "epoch: 27 step: 60 loss: 0.1316223442554474\n",
            "epoch: 27 step: 75 loss: 0.12803632020950317\n",
            "epoch: 28 step: 15 loss: 0.12839175760746002\n",
            "epoch: 28 step: 30 loss: 0.1263311207294464\n",
            "epoch: 28 step: 45 loss: 0.12529058754444122\n",
            "epoch: 28 step: 60 loss: 0.12562444806098938\n",
            "epoch: 28 step: 75 loss: 0.12225250899791718\n",
            "epoch: 29 step: 15 loss: 0.12284111231565475\n",
            "epoch: 29 step: 30 loss: 0.12093250453472137\n",
            "epoch: 29 step: 45 loss: 0.11998265981674194\n",
            "epoch: 29 step: 60 loss: 0.12040643393993378\n",
            "epoch: 29 step: 75 loss: 0.11722719669342041\n",
            "epoch: 30 step: 15 loss: 0.11802111566066742\n",
            "epoch: 30 step: 30 loss: 0.11623992025852203\n",
            "epoch: 30 step: 45 loss: 0.11536138504743576\n",
            "epoch: 30 step: 60 loss: 0.11586606502532959\n",
            "epoch: 30 step: 75 loss: 0.11284413933753967\n",
            "epoch: 31 step: 15 loss: 0.11382059752941132\n",
            "epoch: 31 step: 30 loss: 0.1121501475572586\n",
            "epoch: 31 step: 45 loss: 0.11132588982582092\n",
            "epoch: 31 step: 60 loss: 0.11189329624176025\n",
            "epoch: 31 step: 75 loss: 0.10900802910327911\n",
            "epoch: 32 step: 15 loss: 0.11014385521411896\n",
            "epoch: 32 step: 30 loss: 0.1085699275135994\n",
            "epoch: 32 step: 45 loss: 0.10778963565826416\n",
            "epoch: 32 step: 60 loss: 0.10840950161218643\n",
            "epoch: 32 step: 75 loss: 0.10564374923706055\n",
            "epoch: 33 step: 15 loss: 0.10691767185926437\n",
            "epoch: 33 step: 30 loss: 0.10542884469032288\n",
            "epoch: 33 step: 45 loss: 0.10467962920665741\n",
            "epoch: 33 step: 60 loss: 0.10534295439720154\n",
            "epoch: 33 step: 75 loss: 0.1026844009757042\n",
            "epoch: 34 step: 15 loss: 0.1040768101811409\n",
            "epoch: 34 step: 30 loss: 0.1026649996638298\n",
            "epoch: 34 step: 45 loss: 0.10193830728530884\n",
            "epoch: 34 step: 60 loss: 0.10263679921627045\n",
            "epoch: 34 step: 75 loss: 0.1000693142414093\n",
            "epoch: 35 step: 15 loss: 0.10156310349702835\n",
            "epoch: 35 step: 30 loss: 0.10022194683551788\n",
            "epoch: 35 step: 45 loss: 0.09951300173997879\n",
            "epoch: 35 step: 60 loss: 0.10024036467075348\n",
            "epoch: 35 step: 75 loss: 0.09775131195783615\n",
            "epoch: 36 step: 15 loss: 0.09933722019195557\n",
            "epoch: 36 step: 30 loss: 0.0980544164776802\n",
            "epoch: 36 step: 45 loss: 0.0973581001162529\n",
            "epoch: 36 step: 60 loss: 0.09811223298311234\n",
            "epoch: 36 step: 75 loss: 0.09569042921066284\n",
            "epoch: 37 step: 15 loss: 0.09735842049121857\n",
            "epoch: 37 step: 30 loss: 0.0961284339427948\n",
            "epoch: 37 step: 45 loss: 0.0954386293888092\n",
            "epoch: 37 step: 60 loss: 0.09621407836675644\n",
            "epoch: 37 step: 75 loss: 0.09385411441326141\n",
            "epoch: 38 step: 15 loss: 0.09559686481952667\n",
            "epoch: 38 step: 30 loss: 0.09441103041172028\n",
            "epoch: 38 step: 45 loss: 0.09372414648532867\n",
            "epoch: 38 step: 60 loss: 0.0945170596241951\n",
            "epoch: 38 step: 75 loss: 0.09221107512712479\n",
            "epoch: 39 step: 15 loss: 0.09402383863925934\n",
            "epoch: 39 step: 30 loss: 0.09287609159946442\n",
            "epoch: 39 step: 45 loss: 0.09218789637088776\n",
            "epoch: 39 step: 60 loss: 0.09299495816230774\n",
            "epoch: 39 step: 75 loss: 0.09073787927627563\n",
            "epoch: 40 step: 15 loss: 0.09261378645896912\n",
            "epoch: 40 step: 30 loss: 0.09149850159883499\n",
            "epoch: 40 step: 45 loss: 0.09080767631530762\n",
            "epoch: 40 step: 60 loss: 0.09162716567516327\n",
            "epoch: 40 step: 75 loss: 0.08941411226987839\n",
            "epoch: 41 step: 15 loss: 0.09134615212678909\n",
            "epoch: 41 step: 30 loss: 0.09025907516479492\n",
            "epoch: 41 step: 45 loss: 0.08956509828567505\n",
            "epoch: 41 step: 60 loss: 0.09039489179849625\n",
            "epoch: 41 step: 75 loss: 0.0882192850112915\n",
            "epoch: 42 step: 15 loss: 0.09020136296749115\n",
            "epoch: 42 step: 30 loss: 0.08913961797952652\n",
            "epoch: 42 step: 45 loss: 0.08844151347875595\n",
            "epoch: 42 step: 60 loss: 0.08928227424621582\n",
            "epoch: 42 step: 75 loss: 0.08713884651660919\n",
            "epoch: 43 step: 15 loss: 0.08916802704334259\n",
            "epoch: 43 step: 30 loss: 0.08812713623046875\n",
            "epoch: 43 step: 45 loss: 0.08742499351501465\n",
            "epoch: 43 step: 60 loss: 0.08827318251132965\n",
            "epoch: 43 step: 75 loss: 0.0861583724617958\n",
            "epoch: 44 step: 15 loss: 0.08823150396347046\n",
            "epoch: 44 step: 30 loss: 0.08721006661653519\n",
            "epoch: 44 step: 45 loss: 0.08650234341621399\n",
            "epoch: 44 step: 60 loss: 0.08735798299312592\n",
            "epoch: 44 step: 75 loss: 0.08526842296123505\n",
            "epoch: 45 step: 15 loss: 0.0873817428946495\n",
            "epoch: 45 step: 30 loss: 0.08637624979019165\n",
            "epoch: 45 step: 45 loss: 0.08566164970397949\n",
            "epoch: 45 step: 60 loss: 0.08652512729167938\n",
            "epoch: 45 step: 75 loss: 0.08445847779512405\n",
            "epoch: 46 step: 15 loss: 0.0866067036986351\n",
            "epoch: 46 step: 30 loss: 0.08561710268259048\n",
            "epoch: 46 step: 45 loss: 0.08489561080932617\n",
            "epoch: 46 step: 60 loss: 0.08576328307390213\n",
            "epoch: 46 step: 75 loss: 0.08371740579605103\n",
            "epoch: 47 step: 15 loss: 0.0859009250998497\n",
            "epoch: 47 step: 30 loss: 0.08492332696914673\n",
            "epoch: 47 step: 45 loss: 0.08419489860534668\n",
            "epoch: 47 step: 60 loss: 0.08506663143634796\n",
            "epoch: 47 step: 75 loss: 0.08304010331630707\n",
            "epoch: 48 step: 15 loss: 0.0852557122707367\n",
            "epoch: 48 step: 30 loss: 0.08428925275802612\n",
            "epoch: 48 step: 45 loss: 0.08355346322059631\n",
            "epoch: 48 step: 60 loss: 0.08442845940589905\n",
            "epoch: 48 step: 75 loss: 0.0824194923043251\n",
            "epoch: 49 step: 15 loss: 0.08466457575559616\n",
            "epoch: 49 step: 30 loss: 0.08370906114578247\n",
            "epoch: 49 step: 45 loss: 0.08296512067317963\n",
            "epoch: 49 step: 60 loss: 0.08384235203266144\n",
            "epoch: 49 step: 75 loss: 0.08184901624917984\n",
            "epoch: 50 step: 15 loss: 0.084122434258461\n",
            "epoch: 50 step: 30 loss: 0.08317628502845764\n",
            "epoch: 50 step: 45 loss: 0.08242429792881012\n",
            "epoch: 50 step: 60 loss: 0.08330310881137848\n",
            "epoch: 50 step: 75 loss: 0.0813235193490982\n",
            "epoch: 51 step: 15 loss: 0.08362320810556412\n",
            "epoch: 51 step: 30 loss: 0.08268530666828156\n",
            "epoch: 51 step: 45 loss: 0.08192620426416397\n",
            "epoch: 51 step: 60 loss: 0.0828055664896965\n",
            "epoch: 51 step: 75 loss: 0.08083891868591309\n",
            "epoch: 52 step: 15 loss: 0.08316373825073242\n",
            "epoch: 52 step: 30 loss: 0.08223364502191544\n",
            "epoch: 52 step: 45 loss: 0.0814657062292099\n",
            "epoch: 52 step: 60 loss: 0.08234667778015137\n",
            "epoch: 52 step: 75 loss: 0.080391064286232\n",
            "epoch: 53 step: 15 loss: 0.08273866772651672\n",
            "epoch: 53 step: 30 loss: 0.08181554824113846\n",
            "epoch: 53 step: 45 loss: 0.0810396671295166\n",
            "epoch: 53 step: 60 loss: 0.0819220095872879\n",
            "epoch: 53 step: 75 loss: 0.07997585833072662\n",
            "epoch: 54 step: 15 loss: 0.08234670013189316\n",
            "epoch: 54 step: 30 loss: 0.08142867684364319\n",
            "epoch: 54 step: 45 loss: 0.08064505457878113\n",
            "epoch: 54 step: 60 loss: 0.08152738213539124\n",
            "epoch: 54 step: 75 loss: 0.07959035038948059\n",
            "epoch: 55 step: 15 loss: 0.08198264986276627\n",
            "epoch: 55 step: 30 loss: 0.08106984943151474\n",
            "epoch: 55 step: 45 loss: 0.08027960360050201\n",
            "epoch: 55 step: 60 loss: 0.08116096258163452\n",
            "epoch: 55 step: 75 loss: 0.07923154532909393\n",
            "epoch: 56 step: 15 loss: 0.08164508640766144\n",
            "epoch: 56 step: 30 loss: 0.08073675632476807\n",
            "epoch: 56 step: 45 loss: 0.07993914186954498\n",
            "epoch: 56 step: 60 loss: 0.08082021772861481\n",
            "epoch: 56 step: 75 loss: 0.07889747619628906\n",
            "epoch: 57 step: 15 loss: 0.08133092522621155\n",
            "epoch: 57 step: 30 loss: 0.0804271250963211\n",
            "epoch: 57 step: 45 loss: 0.079621821641922\n",
            "epoch: 57 step: 60 loss: 0.08050175756216049\n",
            "epoch: 57 step: 75 loss: 0.07858599722385406\n",
            "epoch: 58 step: 15 loss: 0.08103873580694199\n",
            "epoch: 58 step: 30 loss: 0.08013866096735\n",
            "epoch: 58 step: 45 loss: 0.07932660728693008\n",
            "epoch: 58 step: 60 loss: 0.08020523190498352\n",
            "epoch: 58 step: 75 loss: 0.07829500734806061\n",
            "epoch: 59 step: 15 loss: 0.08076523244380951\n",
            "epoch: 59 step: 30 loss: 0.07986925542354584\n",
            "epoch: 59 step: 45 loss: 0.07904988527297974\n",
            "epoch: 59 step: 60 loss: 0.07992834597826004\n",
            "epoch: 59 step: 75 loss: 0.07802288234233856\n",
            "epoch: 60 step: 15 loss: 0.08051010966300964\n",
            "epoch: 60 step: 30 loss: 0.0796176940202713\n",
            "epoch: 60 step: 45 loss: 0.07879120856523514\n",
            "epoch: 60 step: 60 loss: 0.07966849952936172\n",
            "epoch: 60 step: 75 loss: 0.07776765525341034\n",
            "epoch: 61 step: 15 loss: 0.08027072250843048\n",
            "epoch: 61 step: 30 loss: 0.07938211411237717\n",
            "epoch: 61 step: 45 loss: 0.0785483866930008\n",
            "epoch: 61 step: 60 loss: 0.079424187541008\n",
            "epoch: 61 step: 75 loss: 0.07752823084592819\n",
            "epoch: 62 step: 15 loss: 0.08004641532897949\n",
            "epoch: 62 step: 30 loss: 0.07916107773780823\n",
            "epoch: 62 step: 45 loss: 0.07832063734531403\n",
            "epoch: 62 step: 60 loss: 0.07919478416442871\n",
            "epoch: 62 step: 75 loss: 0.07730348408222198\n",
            "epoch: 63 step: 15 loss: 0.0798361748456955\n",
            "epoch: 63 step: 30 loss: 0.07895344495773315\n",
            "epoch: 63 step: 45 loss: 0.07810620963573456\n",
            "epoch: 63 step: 60 loss: 0.07897934317588806\n",
            "epoch: 63 step: 75 loss: 0.07709194719791412\n",
            "epoch: 64 step: 15 loss: 0.07963773608207703\n",
            "epoch: 64 step: 30 loss: 0.07875826954841614\n",
            "epoch: 64 step: 45 loss: 0.07790487259626389\n",
            "epoch: 64 step: 60 loss: 0.07877632975578308\n",
            "epoch: 64 step: 75 loss: 0.0768924430012703\n",
            "epoch: 65 step: 15 loss: 0.07945109903812408\n",
            "epoch: 65 step: 30 loss: 0.07857425510883331\n",
            "epoch: 65 step: 45 loss: 0.0777149647474289\n",
            "epoch: 65 step: 60 loss: 0.07858441770076752\n",
            "epoch: 65 step: 75 loss: 0.07670421898365021\n",
            "epoch: 66 step: 15 loss: 0.07927501201629639\n",
            "epoch: 66 step: 30 loss: 0.07840044051408768\n",
            "epoch: 66 step: 45 loss: 0.07753536850214005\n",
            "epoch: 66 step: 60 loss: 0.07840301096439362\n",
            "epoch: 66 step: 75 loss: 0.07652612030506134\n",
            "epoch: 67 step: 15 loss: 0.07910888642072678\n",
            "epoch: 67 step: 30 loss: 0.07823675870895386\n",
            "epoch: 67 step: 45 loss: 0.0773659348487854\n",
            "epoch: 67 step: 60 loss: 0.0782315731048584\n",
            "epoch: 67 step: 75 loss: 0.07635755091905594\n",
            "epoch: 68 step: 15 loss: 0.07895184308290482\n",
            "epoch: 68 step: 30 loss: 0.07808147370815277\n",
            "epoch: 68 step: 45 loss: 0.07720518112182617\n",
            "epoch: 68 step: 60 loss: 0.07806885987520218\n",
            "epoch: 68 step: 75 loss: 0.07619798183441162\n",
            "epoch: 69 step: 15 loss: 0.0788029283285141\n",
            "epoch: 69 step: 30 loss: 0.0779346227645874\n",
            "epoch: 69 step: 45 loss: 0.07705248892307281\n",
            "epoch: 69 step: 60 loss: 0.07791474461555481\n",
            "epoch: 69 step: 75 loss: 0.07604667544364929\n",
            "epoch: 70 step: 15 loss: 0.0786614939570427\n",
            "epoch: 70 step: 30 loss: 0.07779540121555328\n",
            "epoch: 70 step: 45 loss: 0.07690770924091339\n",
            "epoch: 70 step: 60 loss: 0.07776820659637451\n",
            "epoch: 70 step: 75 loss: 0.07590289413928986\n",
            "epoch: 71 step: 15 loss: 0.07852721959352493\n",
            "epoch: 71 step: 30 loss: 0.07766280323266983\n",
            "epoch: 71 step: 45 loss: 0.07677023112773895\n",
            "epoch: 71 step: 60 loss: 0.07762910425662994\n",
            "epoch: 71 step: 75 loss: 0.0757659450173378\n",
            "epoch: 72 step: 15 loss: 0.07839974761009216\n",
            "epoch: 72 step: 30 loss: 0.07753688097000122\n",
            "epoch: 72 step: 45 loss: 0.07663941383361816\n",
            "epoch: 72 step: 60 loss: 0.07749663293361664\n",
            "epoch: 72 step: 75 loss: 0.07563541829586029\n",
            "epoch: 73 step: 15 loss: 0.0782783031463623\n",
            "epoch: 73 step: 30 loss: 0.07741715013980865\n",
            "epoch: 73 step: 45 loss: 0.07651444524526596\n",
            "epoch: 73 step: 60 loss: 0.07737038284540176\n",
            "epoch: 73 step: 75 loss: 0.0755113810300827\n",
            "epoch: 74 step: 15 loss: 0.07816218584775925\n",
            "epoch: 74 step: 30 loss: 0.077302947640419\n",
            "epoch: 74 step: 45 loss: 0.07639565318822861\n",
            "epoch: 74 step: 60 loss: 0.07725005596876144\n",
            "epoch: 74 step: 75 loss: 0.07539236545562744\n",
            "epoch: 75 step: 15 loss: 0.07805189490318298\n",
            "epoch: 75 step: 30 loss: 0.07719402015209198\n",
            "epoch: 75 step: 45 loss: 0.0762818455696106\n",
            "epoch: 75 step: 60 loss: 0.07713499665260315\n",
            "epoch: 75 step: 75 loss: 0.07527858763933182\n",
            "epoch: 76 step: 15 loss: 0.07794636487960815\n",
            "epoch: 76 step: 30 loss: 0.07708963751792908\n",
            "epoch: 76 step: 45 loss: 0.07617340981960297\n",
            "epoch: 76 step: 60 loss: 0.07702484726905823\n",
            "epoch: 76 step: 75 loss: 0.0751698911190033\n",
            "epoch: 77 step: 15 loss: 0.0778454840183258\n",
            "epoch: 77 step: 30 loss: 0.07698998600244522\n",
            "epoch: 77 step: 45 loss: 0.07606934756040573\n",
            "epoch: 77 step: 60 loss: 0.0769195705652237\n",
            "epoch: 77 step: 75 loss: 0.0750657245516777\n",
            "epoch: 78 step: 15 loss: 0.07774953544139862\n",
            "epoch: 78 step: 30 loss: 0.0768943503499031\n",
            "epoch: 78 step: 45 loss: 0.07597023993730545\n",
            "epoch: 78 step: 60 loss: 0.07681865990161896\n",
            "epoch: 78 step: 75 loss: 0.0749659538269043\n",
            "epoch: 79 step: 15 loss: 0.07765708118677139\n",
            "epoch: 79 step: 30 loss: 0.07680264115333557\n",
            "epoch: 79 step: 45 loss: 0.07587502151727676\n",
            "epoch: 79 step: 60 loss: 0.07672179490327835\n",
            "epoch: 79 step: 75 loss: 0.07487020641565323\n",
            "epoch: 80 step: 15 loss: 0.07756834477186203\n",
            "epoch: 80 step: 30 loss: 0.07671479880809784\n",
            "epoch: 80 step: 45 loss: 0.07578346878290176\n",
            "epoch: 80 step: 60 loss: 0.07662880420684814\n",
            "epoch: 80 step: 75 loss: 0.07477803528308868\n",
            "epoch: 81 step: 15 loss: 0.07748328149318695\n",
            "epoch: 81 step: 30 loss: 0.07663014531135559\n",
            "epoch: 81 step: 45 loss: 0.07569590210914612\n",
            "epoch: 81 step: 60 loss: 0.0765393078327179\n",
            "epoch: 81 step: 75 loss: 0.07468968629837036\n",
            "epoch: 82 step: 15 loss: 0.07740128040313721\n",
            "epoch: 82 step: 30 loss: 0.07654916495084763\n",
            "epoch: 82 step: 45 loss: 0.0756111592054367\n",
            "epoch: 82 step: 60 loss: 0.07645310461521149\n",
            "epoch: 82 step: 75 loss: 0.07460483908653259\n",
            "epoch: 83 step: 15 loss: 0.07732247561216354\n",
            "epoch: 83 step: 30 loss: 0.07647134363651276\n",
            "epoch: 83 step: 45 loss: 0.07552959024906158\n",
            "epoch: 83 step: 60 loss: 0.07637029141187668\n",
            "epoch: 83 step: 75 loss: 0.07452269643545151\n",
            "epoch: 84 step: 15 loss: 0.07724630832672119\n",
            "epoch: 84 step: 30 loss: 0.0763959288597107\n",
            "epoch: 84 step: 45 loss: 0.07545152306556702\n",
            "epoch: 84 step: 60 loss: 0.07629033923149109\n",
            "epoch: 84 step: 75 loss: 0.07444380223751068\n",
            "epoch: 85 step: 15 loss: 0.07717324793338776\n",
            "epoch: 85 step: 30 loss: 0.07632343471050262\n",
            "epoch: 85 step: 45 loss: 0.07537601888179779\n",
            "epoch: 85 step: 60 loss: 0.07621336728334427\n",
            "epoch: 85 step: 75 loss: 0.07436756044626236\n",
            "epoch: 86 step: 15 loss: 0.07710276544094086\n",
            "epoch: 86 step: 30 loss: 0.07625355571508408\n",
            "epoch: 86 step: 45 loss: 0.0753033384680748\n",
            "epoch: 86 step: 60 loss: 0.07613906264305115\n",
            "epoch: 86 step: 75 loss: 0.07429394125938416\n",
            "epoch: 87 step: 15 loss: 0.07703475654125214\n",
            "epoch: 87 step: 30 loss: 0.07618594169616699\n",
            "epoch: 87 step: 45 loss: 0.07523311674594879\n",
            "epoch: 87 step: 60 loss: 0.07606655359268188\n",
            "epoch: 87 step: 75 loss: 0.07422292232513428\n",
            "epoch: 88 step: 15 loss: 0.07696901261806488\n",
            "epoch: 88 step: 30 loss: 0.07612041383981705\n",
            "epoch: 88 step: 45 loss: 0.07516522705554962\n",
            "epoch: 88 step: 60 loss: 0.07599716633558273\n",
            "epoch: 88 step: 75 loss: 0.07415419816970825\n",
            "epoch: 89 step: 15 loss: 0.07690534740686417\n",
            "epoch: 89 step: 30 loss: 0.07605748623609543\n",
            "epoch: 89 step: 45 loss: 0.07509914040565491\n",
            "epoch: 89 step: 60 loss: 0.0759299248456955\n",
            "epoch: 89 step: 75 loss: 0.07408773899078369\n",
            "epoch: 90 step: 15 loss: 0.07684416323900223\n",
            "epoch: 90 step: 30 loss: 0.07599617540836334\n",
            "epoch: 90 step: 45 loss: 0.07503533363342285\n",
            "epoch: 90 step: 60 loss: 0.07586444914340973\n",
            "epoch: 90 step: 75 loss: 0.07402323186397552\n",
            "epoch: 91 step: 15 loss: 0.07678452134132385\n",
            "epoch: 91 step: 30 loss: 0.07593704015016556\n",
            "epoch: 91 step: 45 loss: 0.0749739557504654\n",
            "epoch: 91 step: 60 loss: 0.0758010596036911\n",
            "epoch: 91 step: 75 loss: 0.073961041867733\n",
            "epoch: 92 step: 15 loss: 0.0767267495393753\n",
            "epoch: 92 step: 30 loss: 0.07587948441505432\n",
            "epoch: 92 step: 45 loss: 0.07491416484117508\n",
            "epoch: 92 step: 60 loss: 0.0757400393486023\n",
            "epoch: 92 step: 75 loss: 0.0739002674818039\n",
            "epoch: 93 step: 15 loss: 0.07667089998722076\n",
            "epoch: 93 step: 30 loss: 0.07582352310419083\n",
            "epoch: 93 step: 45 loss: 0.07485634833574295\n",
            "epoch: 93 step: 60 loss: 0.07568055391311646\n",
            "epoch: 93 step: 75 loss: 0.07384127378463745\n",
            "epoch: 94 step: 15 loss: 0.07661658525466919\n",
            "epoch: 94 step: 30 loss: 0.07576948404312134\n",
            "epoch: 94 step: 45 loss: 0.0748002678155899\n",
            "epoch: 94 step: 60 loss: 0.07562245428562164\n",
            "epoch: 94 step: 75 loss: 0.07378427684307098\n",
            "epoch: 95 step: 15 loss: 0.0765637755393982\n",
            "epoch: 95 step: 30 loss: 0.07571716606616974\n",
            "epoch: 95 step: 45 loss: 0.07474607974290848\n",
            "epoch: 95 step: 60 loss: 0.07556626200675964\n",
            "epoch: 95 step: 75 loss: 0.07372845709323883\n",
            "epoch: 96 step: 15 loss: 0.07651250064373016\n",
            "epoch: 96 step: 30 loss: 0.07566574960947037\n",
            "epoch: 96 step: 45 loss: 0.07469295710325241\n",
            "epoch: 96 step: 60 loss: 0.07551199942827225\n",
            "epoch: 96 step: 75 loss: 0.07367435097694397\n",
            "epoch: 97 step: 15 loss: 0.0764622762799263\n",
            "epoch: 97 step: 30 loss: 0.07561605423688889\n",
            "epoch: 97 step: 45 loss: 0.07464179396629333\n",
            "epoch: 97 step: 60 loss: 0.07545892894268036\n",
            "epoch: 97 step: 75 loss: 0.07362192124128342\n",
            "epoch: 98 step: 15 loss: 0.0764133408665657\n",
            "epoch: 98 step: 30 loss: 0.07556775957345963\n",
            "epoch: 98 step: 45 loss: 0.07459163665771484\n",
            "epoch: 98 step: 60 loss: 0.07540694624185562\n",
            "epoch: 98 step: 75 loss: 0.07357095181941986\n",
            "epoch: 99 step: 15 loss: 0.07636593282222748\n",
            "epoch: 99 step: 30 loss: 0.07552092522382736\n",
            "epoch: 99 step: 45 loss: 0.07454288750886917\n",
            "epoch: 99 step: 60 loss: 0.07535630464553833\n",
            "epoch: 99 step: 75 loss: 0.07352103292942047\n",
            "epoch: 100 step: 15 loss: 0.07631926238536835\n",
            "epoch: 100 step: 30 loss: 0.07547514885663986\n",
            "epoch: 100 step: 45 loss: 0.07449539750814438\n",
            "epoch: 100 step: 60 loss: 0.07530711591243744\n",
            "epoch: 100 step: 75 loss: 0.07347303628921509\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_uKOpe8IGJk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9a8bc6f0-992e-4390-8253-888019910e19"
      },
      "source": [
        "\"\"\"TEST\"\"\"\n",
        "def get_char_count(arg1):\n",
        "    c0 = ALL_CHAR_SET[np.argmax(arg1.cpu().tolist()[0:ALL_CHAR_SET_LEN])]\n",
        "    c1 = ALL_CHAR_SET[np.argmax(arg1.cpu().tolist()[ALL_CHAR_SET_LEN:ALL_CHAR_SET_LEN*2])]\n",
        "    c2 = ALL_CHAR_SET[np.argmax(arg1.cpu().tolist()[ALL_CHAR_SET_LEN*2:ALL_CHAR_SET_LEN*3])]\n",
        "    c3 = ALL_CHAR_SET[np.argmax(arg1.cpu().tolist()[ALL_CHAR_SET_LEN*3:ALL_CHAR_SET_LEN*4])]\n",
        "    c4 = ALL_CHAR_SET[np.argmax(arg1.cpu().tolist()[ALL_CHAR_SET_LEN*4:ALL_CHAR_SET_LEN*5])]\n",
        "    c5 = ALL_CHAR_SET[np.argmax(arg1.cpu().tolist()[ALL_CHAR_SET_LEN*5:ALL_CHAR_SET_LEN*6])]\n",
        "    c6 = ALL_CHAR_SET[np.argmax(arg1.cpu().tolist()[ALL_CHAR_SET_LEN*6:ALL_CHAR_SET_LEN*7])]\n",
        "    return c0, c1, c2,c3, c4, c5, c6 \n",
        " \n",
        "\n",
        "\n",
        "char_correct = 0\n",
        "word_correct = 0\n",
        "total = 0\n",
        "\n",
        "betternet.eval()\n",
        "lstm.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for step, (img, label_oh, label) in enumerate(test_dl):\n",
        "        char_count =0\n",
        "        img = Variable(img).cuda()\n",
        "        label_oh = Variable(label_oh.float()).cuda()\n",
        "        pred = betternet(img)\n",
        "     #   pred = lstm(pred, label_oh) # outputs = lstm(feature, ...) 수정해야\n",
        "        label_len = label[0]\n",
        "        pred = pred.squeeze(0)\n",
        "        label_oh = label_oh.squeeze(0)\n",
        "        \n",
        "        c0,c1,c2,c3,c4,c5,c6 = get_char_count(pred.squeeze()) \n",
        "        d0,d1,d2,d3,d4,d5,d6 = get_char_count(label_oh) \n",
        "         \n",
        "        c = '%s%s%s%s%s%s%s' % (c0, c1, c2, c3, c4, c5, c6)\n",
        "        d = '%s%s%s%s%s%s%s' % (d0, d1, d2, d3, d4, d5, d6)\n",
        "    \n",
        "        char_count += (c0==d0)+(c1==d1)+(c2==d2)+(c3==d3)+(c4==d4)+(c5==d5)+(c6==d6)\n",
        "        char_correct += char_count\n",
        "\n",
        "        if(bool(str(label[0]) in str(c))):\n",
        "            word_correct+=1\n",
        "\n",
        "        total += 1\n",
        "       \n",
        "print(100/7*char_correct/total)\n",
        "print(100*word_correct/total)\n",
        "\"\"\"END TEST\"\"\""
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "37.1\n",
            "0.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'END TEST'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ174540oY2-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(betternet.state_dict(), './model.pth')"
      ],
      "execution_count": 13,
      "outputs": []
    }
  ]
}